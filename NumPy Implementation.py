# -*- coding: utf-8 -*-
"""MvsN.ipynb

Automatically generated by Colaboratory.
"""

import matplotlib.pyplot as plt
import cv2 
import numpy as np
import os

np.random.seed(1)

from zipfile import ZipFile 

with ZipFile("Stage_2.zip",'r') as zipfile:
    zipfile.extractall()
    print('Files loaded')

memes_dir = "Stage_2/Memes"
notes_dir = "Stage_2/Notes"

def var_store(path,label):
  images = []
  labels = []
  for file in os.listdir(path):
    img = cv2.imread(path + "/" + file,0)
    img = cv2.resize(img,(100,100))
    images.append(img)
    labels.append(label)
  return images,labels

memes, memesout = var_store(memes_dir, 1)
notes, notesout = var_store(notes_dir, 0)
memes,memesout = np.array(memes),np.array(memesout)
notes,notesout = np.array(notes),np.array(notesout)
memes = memes.reshape(800,100,100,1)
notes = notes.reshape(800,100,100,1)
#memes[56:]
#notes[56]

memewhole = list(zip(memes, memesout))
notewhole = list(zip(notes, notesout))

temp = []
temp = memewhole[0:750] + notewhole[0:750]
np.random.shuffle(temp)
train_X, train_Y = list(zip(*temp))
temp.clear()
temp = memewhole[750:780] + notewhole[750:780]
np.random.shuffle(temp)
dev_X, dev_Y = list(zip(*temp))
temp.clear()
temp = memewhole[780:] + notewhole[780:]
np.random.shuffle(temp)
test_X, test_Y = list(zip(*temp))

train_X = np.array(train_X)
dev_X = np.array(dev_X)
test_X = np.array(test_X)

train_Y = np.array(train_Y)
dev_Y = np.array(dev_Y)
test_Y = np.array(test_Y)

#np.shape(train_X)

train_X = train_X.reshape(1500,-1).T
dev_X = dev_X.reshape(60,-1).T
test_X = test_X.reshape(40,-1).T

train_Y = train_Y.reshape(1,1500)
dev_Y = dev_Y.reshape(1,60)
test_Y = test_Y.reshape(1,40)

train_X = train_X / 255.0
dev_X = dev_X / 255.0 
test_X = test_X / 255.0

#plt.imshow(train_X[])
#np.shape(train_X)

def relu(z):
  x = np.maximum(0,z)
  return x

def sigmoid(z):
  x = 1/(1 + np.exp(-z)) 
  return x
  
def Relu_der(z):
    x = (z + np.abs(z))/(2*np.abs(z))
    return x

def parameter_creation(layer_dims):
  parameters={}
  for i in range(1,len(layer_dims)):
    parameters['W' + str(i)] = np.random.randn(layer_dims[i],layer_dims[i - 1])*0.01
    parameters['b' + str(i)] = np.zeros((layer_dims[i],1))
  return parameters

def cost(AL,Y):
  m = Y.shape[1]
  cost = -np.sum((1 - Y)*np.log(1 - AL) + (Y * np.log(AL)))/m
  return cost

def forward_propagation(X, parameters):
    temp = {}
    l = int(len(parameters)/2)
    temp["A0"] = X
    for i in range(1 , l + 1):
      temp["Z" + str(i)] = np.dot(parameters["W" + str(i)],temp["A" + str(i - 1)]) + parameters["b" + str(i)]
      
      temp["A" + str(i)] = relu(temp["Z" + str(i)])
        
    temp["A" + str(l)] = sigmoid(temp["Z" + str(l)])
            
    return temp

def back_propagation(X, Y, parameters, temp):
  m = Y.shape[1]
  l = len(parameters)//2
  temp2 = {}
  dZ = temp["A" + str(l)] - Y
  temp2["dW" + str(l)] = (1/m)*np.dot(dZ, temp["A" + str(l-1)].T)
  temp2["db" + str(l)] = (1/m)*np.sum(dZ, axis=1, keepdims = True)
  for i in range(l-1,0,-1):
    dZ = np.dot(parameters["W" + str(i+1)].T,dZ)*Relu_der(temp["Z" + str(i)])
    temp2["dW" + str(i)] = (1/m)*np.dot(dZ, temp["A" + str(i - 1)].T)
    temp2["db" + str(i)] = (1/m)*np.sum(dZ, axis=1, keepdims=True)
  return temp2

def update(parameters, temp2, lr):
  l = len(parameters)//2
  for i in range(1,l+1):
    parameters['W'+str(i)] -= lr*temp2['dW'+str(i)]
    parameters['b'+str(i)] -= lr*temp2['db'+str(i)]
      
  return parameters

layers = [10000,1500, 50, 1]
parameters = parameter_creation(layers)

costs = []
for epoch in range(10):
  temp = forward_propagation(train_X, parameters)
  c = cost(temp['A'+str(len(layers) - 1)],train_Y)
  costs.append(c)
  print("COST AFTER EPOCH" , (epoch+1) , ":" , c)
  grads = back_propagation(train_X, train_Y, parameters, temp)

  parameters = update(parameters, grads, 0.21)

plt.plot(range(1,11),costs)
plt.xlabel("EPOCHS")
plt.ylabel("COST")
plt.title("LEARNING RATE 0.8")
plt.show()

temp = forward_propagation(train_X, parameters)
np.shape(temp['A' + str(len(layers) - 1)])
Y = temp['A' + str(len(layers) - 1)]
correct = 0
memes =0
notes =0
for i in range(0,len(train_Y[0])):
  if(Y[:,i] < 0.5):
    Y[:,i] = 0
  else: Y[:,i] = 1 
  if(Y[:,i] == train_Y[:,i] == 1):
    correct += 1
    memes += 1
  elif(Y[:,i] == train_Y[:,i] == 0):
    correct += 1
    notes += 1
print(correct*10/150)
print(memes,notes)

temp = forward_propagation(dev_X, parameters)
np.shape(temp['A' + str(len(layers) - 1)])
Y = temp['A' + str(len(layers) - 1)]
correct = 0
memes =0
notes =0
for i in range(0,len(dev_Y[0])):
  if(Y[:,i] < 0.5):
    Y[:,i] = 0
  else: Y[:,i] = 1 
  if(Y[:,i] == dev_Y[:,i] == 1):
    correct += 1
    memes += 1
  elif(Y[:,i] == dev_Y[:,i] == 0):
    correct += 1
    notes += 1
print(correct*10/6)
print(memes,notes)

temp = forward_propagation(test_X, parameters)
np.shape(temp['A' + str(len(layers) - 1)])
Y = temp['A' + str(len(layers) - 1)]
correct = 0
memes =0
notes =0
for i in range(0,len(test_Y[0])):
  if(Y[:,i] < 0.5):
    Y[:,i] = 0
  else: Y[:,i] = 1 
  if(Y[:,i] == test_Y[:,i] == 1):
    correct += 1
    memes += 1
  elif(Y[:,i] == test_Y[:,i] == 0):
    correct += 1
    notes += 1
print(correct*10/4)
print(memes,notes)

layers = [10000, 1000, 50, 1]
parameters = parameter_creation(layers)

Batchsize = 25
costs = []
for epoch in range(10):
  for i in range(60):
    temp = forward_propagation(train_X[:,i:i + Batchsize], parameters)
    c = cost(temp['A'+str(len(layers) - 1)],train_Y[:,i:i + Batchsize])
    costs.append(c)
    grads = back_propagation(train_X[:,i:i + Batchsize], train_Y[:,i:i + Batchsize], parameters, temp)

    parameters = update(parameters, grads, 0.008)
  print("COST AFTER EPOCH" , (epoch+1) , ":" , c)

plt.plot(range(1,601),costs)
plt.xlabel("ITERATION")
plt.ylabel("COST")
plt.title("LEARNING RATE 0.08")
plt.show()

temp = forward_propagation(train_X, parameters)
np.shape(temp['A' + str(len(layers) - 1)])
Y = temp['A' + str(len(layers) - 1)]
correct = 0
memes =0
notes =0
for i in range(0,len(train_Y[0])):
  if(Y[:,i] < 0.5):
    Y[:,i] = 0
  else: Y[:,i] = 1 
  if(Y[:,i] == train_Y[:,i] == 1):
    correct += 1
    memes += 1
  elif(Y[:,i] == train_Y[:,i] == 0):
    correct += 1
    notes += 1
print(correct*10/150)
print(memes,notes)

temp = forward_propagation(dev_X, parameters)
np.shape(temp['A' + str(len(layers) - 1)])
Y = temp['A' + str(len(layers) - 1)]
correct = 0
memes =0
notes =0
for i in range(0,len(dev_Y[0])):
  if(Y[:,i] < 0.5):
    Y[:,i] = 0
  else: Y[:,i] = 1 
  if(Y[:,i] == dev_Y[:,i] == 1):
    correct += 1
    memes += 1
  elif(Y[:,i] == dev_Y[:,i] == 0):
    correct += 1
    notes += 1
print(correct*10/6)
print(memes,notes)

temp = forward_propagation(test_X, parameters)
np.shape(temp['A' + str(len(layers) - 1)])
Y = temp['A' + str(len(layers) - 1)]
correct = 0
memes =0
notes =0
for i in range(0,len(test_Y[0])):
  if(Y[:,i] < 0.5):
    Y[:,i] = 0
  else: Y[:,i] = 1 
  if(Y[:,i] == test_Y[:,i] == 1):
    correct += 1
    memes += 1
  elif(Y[:,i] == test_Y[:,i] == 0):
    correct += 1
    notes += 1
print(correct*10/4)
print(memes,notes)

parameters1=parameters
